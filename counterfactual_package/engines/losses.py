"""
Module for calculating various loss functions and penalties in machine learning models.

This module includes implementations for gradient penalty, continuous and categorical
losses, divergence loss, masked divergence loss, focal loss, and realism loss.
"""

from typing import Callable, Optional, Union

import torch
import torch.nn as nn
import torch.nn.functional as F

from ..data.metadata import DataTypes, MetaData
from ..data.utils import apply_mask, split_torch_tensor_by_columns


def calc_gradient_penalty(
    real_data: torch.Tensor,
    fake_data: torch.Tensor,
    discriminator: nn.Module,
    device: Union[str, torch.device] = "cpu",
    pac: int = 10,
    gradient_penalty_influence: int = 10,
) -> torch.Tensor:
    """
    Compute the gradient penalty for WGAN-GP.

    Args:
        real_data (torch.Tensor): Real data samples.
        fake_data (torch.Tensor): Fake data samples generated by the generator.
        discriminator (nn.Module): Discriminator model.
        device (Union[str, torch.device]): Device to run the computation on.
        pac (int): Number of samples per pac.
        gradient_penalty_influence (int): Influence factor for gradient penalty.

    Returns:
        torch.Tensor: Computed gradient penalty.
    """
    alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)
    alpha = alpha.repeat(1, pac, real_data.size(1)).view(-1, real_data.size(1))

    interpolates = alpha * real_data + ((1 - alpha) * fake_data)

    if isinstance(device, str):
        device = torch.device(device)

    with torch.set_grad_enabled(True):
        interpolates.requires_grad_(True)
        disc_interpolates = discriminator(interpolates)

        # calculate the gradient with as variables the interpolation vars
        # (only on the inputs vs all intermediates)
        gradients = torch.autograd.grad(
            outputs=disc_interpolates,
            inputs=interpolates,
            grad_outputs=torch.ones(disc_interpolates.size(), device=device),
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
        )[0]

    # retain the 2D struct of the seperate pacs + calculate the L2 norm - 1
    gradients_view = gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1

    # calculate the mean over the values of seperate pacs times the gp influence
    gradient_penalty = ((gradients_view) ** 2).mean() * gradient_penalty_influence

    return gradient_penalty


def continuous_loss(
    cont_predicted: torch.Tensor,
    cont_target: torch.Tensor,
    float_index: int = 0,
    float_influence: float = 1,
    float_loss_fn: Callable = nn.MSELoss(reduction="none"),
) -> torch.Tensor:
    """
    Compute the loss for continuous variables.

    Args:
        cont_predicted (torch.Tensor): Predicted continuous values.
        cont_target (torch.Tensor): Target continuous values.
        float_index (int): Index of the float variable in the target tensor.
        float_influence (float): Influence factor for the float loss.
        float_loss_fn (Callable): Loss function for continuous variables.

    Returns:
        torch.Tensor: Computed continuous loss.
    """
    float_target, mode_target = split_torch_tensor_by_columns(
        cont_target, [float_index]
    )
    float_pred, mode_pred = split_torch_tensor_by_columns(cont_predicted, [float_index])

    float_loss = float_loss_fn(float_target, float_pred)

    if mode_pred.nelement() != 0:
        mode_loss = F.cross_entropy(
            mode_pred, torch.argmax(mode_target, dim=1), reduction="none"
        ).unsqueeze(dim=1)
    else:
        mode_loss = torch.tensor(0)

    return mode_loss + float_influence * float_loss


def categorical_loss(
    cat_predicted: torch.Tensor, cat_target: torch.Tensor
) -> torch.Tensor:
    """
    Compute the loss for categorical variables.

    Args:
        cat_predicted (torch.Tensor): Predicted categorical values.
        cat_target (torch.Tensor): Target categorical values.

    Returns:
        torch.Tensor: Computed categorical loss.
    """
    cat_loss = F.cross_entropy(
        cat_predicted, torch.argmax(cat_target, dim=1), reduction="none"
    ).unsqueeze(dim=1)

    return cat_loss


def calc_divergence_loss(
    predicted: torch.Tensor,
    target: torch.Tensor,
    metadata: MetaData,
    continous_float_influence: float = 1,
    col_weights: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """
    Compute the divergence loss for a given predicted and target tensor.

    Args:
        predicted (torch.Tensor): Predicted values.
        target (torch.Tensor): Target values.
        metadata (MetaData): Metadata describing the data.
        continous_float_influence (float): Influence factor for continuous float loss.
        col_weights (Optional[torch.Tensor]): Column weights for the loss calculation.

    Returns:
        torch.Tensor: Computed divergence loss.
    """
    loss = []
    start = 0

    for column_meta in metadata:
        end = start + column_meta.output_dimension
        pred_col, _ = split_torch_tensor_by_columns(predicted, range(start, end))
        target_col, _ = split_torch_tensor_by_columns(target, range(start, end))

        if column_meta.column_type == DataTypes.CONTINUOUS:
            col_loss = continuous_loss(
                pred_col, target_col, float_influence=continous_float_influence
            )
        else:
            col_loss = categorical_loss(pred_col, target_col)

        loss.append(col_loss)
        start = end

    if col_weights is not None:
        return torch.concat(loss, dim=1) * col_weights
    else:
        return torch.concat(loss, dim=1)


def masked_divergence_loss(
    predicted: torch.Tensor,
    target: torch.Tensor,
    col_mask: torch.Tensor,
    metadata: MetaData,
    continous_float_influence: float = 1,
    col_weights: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """
    Compute the masked divergence loss for columns indicated True in the mask.

    Args:
        predicted (torch.Tensor): Predicted values.
        target (torch.Tensor): Target values.
        col_mask (torch.Tensor): Mask indicating columns to consider for loss.
        metadata (MetaData): Metadata describing the data.
        continous_float_influence (float): Influence factor for continuous float loss.
        col_weights (Optional[torch.Tensor]): Column weights for the loss calculation.

    Returns:
        torch.Tensor: Computed masked divergence loss.
    """
    if col_mask.shape != (predicted.shape[0], metadata.num_columns()):
        raise ValueError(
            f"The mask shape {col_mask.shape} is incorrect. "
            "It should be (num_samples, num_columns)="
            f"({predicted.shape[0]}, {metadata.num_columns()})"
        )

    loss_concat = calc_divergence_loss(
        predicted=predicted,
        target=target,
        metadata=metadata,
        continous_float_influence=continous_float_influence,
        col_weights=col_weights,
    )

    masked_loss = apply_mask(loss_concat, col_mask)
    return masked_loss


class FocalLoss(nn.Module):
    def __init__(
        self,
        weights: Optional[torch.Tensor] = None,
        gamma: float = 2.0,
        reduction: str = "mean",
    ):
        """
        Focal Loss implementation.

        Args:
            weights (Optional[torch.Tensor]): Class weights.
            gamma (float): Focusing parameter.
            reduction (str): Reduction method to apply ('mean', 'sum', or 'none').
        """
        super(FocalLoss, self).__init__()
        self.weights = weights
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for Focal Loss.

        Args:
            inputs (torch.Tensor): Predicted values.
            targets (torch.Tensor): Target values.

        Returns:
            torch.Tensor: Computed Focal Loss.
        """
        targets = targets.view(-1, 1)
        one_hot_targets = torch.zeros_like(inputs).scatter_(1, targets, 1)

        # Calculate log softmax and softmax
        log_softmax = F.log_softmax(inputs, dim=1)
        softmax = torch.exp(log_softmax)

        # Calculate focal loss
        focal_weight = (1 - softmax).pow(self.gamma)
        focal_loss = -focal_weight * log_softmax

        if self.weights is not None:
            focal_loss *= self.weights

        # Gather the loss corresponding to the true class
        loss = (one_hot_targets * focal_loss).sum(dim=1)

        if self.reduction == "mean":
            return loss.mean()
        elif self.reduction == "sum":
            return loss.sum()
        else:
            return loss


class RealismLoss(nn.Module):
    def __init__(self, ctgan, target_name: str):
        """
        Realism Loss implementation for CTGAN models.

        Args:
            ctgan: CTGAN model instance.
            target_name (str): Name of the target column.
        """
        super(RealismLoss, self).__init__()
        self.ctgan = ctgan
        self.target_name = target_name

    def to(self, device: torch.device) -> "RealismLoss":  # type: ignore
        """
        Move the model to the specified device.

        Args:
            device (torch.device): Device to move the model to.

        Returns:
            RealismLoss: Self instance for method chaining.
        """
        self.ctgan.to(device)
        return self

    def forward(self, counterfactuals: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for Realism Loss.

        Args:
            counterfactuals (torch.Tensor): Counterfactual examples.

        Returns:
            torch.Tensor: Computed realism loss.
        """
        target_idxs = self.ctgan.metadata.column_to_transformed_idxs(self.target_name)
        target_idx = self.ctgan.metadata.column_to_idx(
            self.target_name, only_categories=True
        )
        cond_vector_dim = self.ctgan.sampler.dim_conditional_vector()
        cond_vector_start = self.ctgan.sampler.categorical_column_cond_start_id[
            target_idx
        ]

        value_ids = torch.argmax(counterfactuals[:, target_idxs], dim=1)

        cond_vecs = torch.zeros(
            (len(counterfactuals), cond_vector_dim), device=counterfactuals.device
        )
        cond_vecs[torch.arange(len(counterfactuals)), cond_vector_start + value_ids] = (
            1.0
        )

        discriminator_input = torch.cat([counterfactuals, cond_vecs], dim=1)
        discriminator_input_repeated = discriminator_input.repeat_interleave(
            self.ctgan.config["pac"], dim=0
        )

        return self.ctgan.model.discriminator(discriminator_input_repeated).squeeze()
